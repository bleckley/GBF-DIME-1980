{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding 1980 data, using the _Geographic Base File/Dual Independent Map Encoding (GBF/DIME), 1980_\n",
    "\n",
    "This program uses the [Geographic Base File/Dual Independent Map Encoding (GBF/DIME), 1980](https://doi.org/10.3886/ICPSR08378.v1) data which were formatted using the code here: https://github.com/bleckley/GBF-DIME-1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_usaddress'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff353de0b8c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#Import necessary modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_usaddress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_usaddress'"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "#\n",
    "#         SECTION 1: Setting up all the resources needed for the later work\n",
    "#\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "#Import necessary modules\n",
    "import pandas as pd\n",
    "import pandas_usaddress #Note: if not installed: pip install pandas-usaddress\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Defining a function to determine if an address is on the right (even) or left (odd) side of the street\n",
    "def side_function(row):\n",
    "    if type(row['AddressNumber']) == float : \n",
    "        if np.isnan(row['AddressNumber']) :\n",
    "            val = None\n",
    "    elif row['AddressNumber'] is None :\n",
    "        val = None\n",
    "    elif row['AddressNumber'].strip()[-1] in ('0','2','4','6','8') :\n",
    "        val = 'Right'\n",
    "    elif row['AddressNumber'].strip()[-1] in ('1','3','5','7','9') :\n",
    "        val = 'Left'\n",
    "    else : val = 'Error'\n",
    "    return val\n",
    "\n",
    "\n",
    "#--------------Creating dictionary of state FIPS codes-----------------\n",
    "\n",
    "state_fips = {\n",
    "    'WA': '53', 'DE': '10', 'DC': '11', 'WI': '55', 'WV': '54', 'HI': '15',\n",
    "    'FL': '12', 'WY': '56', 'PR': '72', 'NJ': '34', 'NM': '35', 'TX': '48',\n",
    "    'LA': '22', 'NC': '37', 'ND': '38', 'NE': '31', 'TN': '47', 'NY': '36',\n",
    "    'PA': '42', 'AK': '02', 'NV': '32', 'NH': '33', 'VA': '51', 'CO': '08',\n",
    "    'CA': '06', 'AL': '01', 'AR': '05', 'VT': '50', 'IL': '17', 'GA': '13',\n",
    "    'IN': '18', 'IA': '19', 'MA': '25', 'AZ': '04', 'ID': '16', 'CT': '09',\n",
    "    'ME': '23', 'MD': '24', 'OK': '40', 'OH': '39', 'UT': '49', 'MO': '29',\n",
    "    'MN': '27', 'MI': '26', 'RI': '44', 'KS': '20', 'MT': '30', 'MS': '28',\n",
    "    'SC': '45', 'KY': '21', 'OR': '41', 'SD': '46', 'AS': '60', 'GU': '66',\n",
    "    'MP': '69', 'PR': '72', 'VI': '78'\n",
    "}\n",
    "\n",
    "\n",
    "#----------CREATING DATAFRAME FOR PLACE CODES ----------------------\n",
    "\n",
    "#Reading text file from url to dataframe \n",
    "#Designating datatype to ensure all data are strings to retain leading zeros\n",
    "place_df = pd.read_fwf('https://www2.census.gov/geo/tiger/PREVGENZ/pl/us_places.txt', dtype={'CENSUS': object, 'FIPS': object, 'NAME': object})\n",
    "\n",
    "#Use regular expression to remove superfluous words at the end.\n",
    "place_df.replace(' (?:city|town|village|CDP|comunidad|borough|zona urbana).*?$','',regex=True, inplace = True)\n",
    "\n",
    "#Dropping unused column\n",
    "place_df = place_df.drop(['FIPS'], axis=1)\n",
    "\n",
    "\n",
    "#Creating State and Place columns from CENSUS column\n",
    "place_df['STATE'] = place_df['CENSUS'].str[:2]\n",
    "place_df['PLACE'] = place_df['CENSUS'].str[2:]\n",
    "\n",
    "#Converting city names to uppercase\n",
    "place_df['NAME'] = place_df['NAME'].str.upper()\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#\n",
    "#         SECTION 2: Creating the dataframe from address source file\n",
    "#\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#------------Loading the address data--------------------------\n",
    "\n",
    "#Note: the needs of this section will change depending on the source file format and layout\n",
    "\n",
    "#load dataframe\n",
    "df = pd.read_csv('sourcefilename')\n",
    "#df.columns = ['streetaddress', 'citystzip'] #--Assign column names if a text file without headers\n",
    "\n",
    "#Parse to columns\n",
    "df = pandas_usaddress.tag(df, ['Address', 'City', 'State', 'Zip'], granularity='medium', standardize=True)\n",
    "\n",
    "#Convert string columns to upper case\n",
    "df['PlaceName'] = df['PlaceName'].str.upper()\n",
    "df['StateName'] = df['StateName'].str.upper()\n",
    "df['StreetName'] = df['StreetName'].str.upper()\n",
    "df['StreetNamePreDirectional'] = df['StreetNamePreDirectional'].str.upper()\n",
    "df['StreetNamePostDirectional'] = df['StreetNamePostDirectional'].str.upper()\n",
    "df['StreetNamePrefix'] = df['StreetNamePrefix'].str.upper()\n",
    "df['StreetNameSuffix'] = df['StreetNameSuffix'].str.upper()\n",
    "\n",
    "#Adding a unique ID so that later we can see records excluded (since index can change)\n",
    "df.insert(0, 'New_ID', range(0, len(df)))\n",
    "\n",
    "\n",
    "#-------------------------TEMP FIX-----------------------------------\n",
    "\n",
    "#NOTE: I'm seeing errors in the StreetNameSuffix and PlaceName fields, \n",
    "#where City was parsed into \"Ann\" and \"Arbor,\" respectively.\n",
    "#I submitted an error report to the developers and will use the following work-around for now...\n",
    "\n",
    "#Deleting erroneous \"Ann\" from StreetNameSuffix column.\n",
    "df['StreetNameSuffix'].replace({'ANN': ''}, inplace = True)\n",
    "#Recoding \"ARBOR\" to \"ANN ARBOR\" in PlaceName column.\n",
    "df['PlaceName'].replace('ARBOR', 'ANN ARBOR', inplace = True)\n",
    "\n",
    "#This may need to be expanded to other multi-word place names, including major cities like New York and Los Angeles.\n",
    "#It will be important to spot-check the dataframe to see how those place names get parsed.\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "#Add State FIPS code to dataframe\n",
    "df['STFIPS'] = df['StateName'].map(state_fips)\n",
    "\n",
    "#Add Census place code to dataframe\n",
    "df = df.merge(place_df, left_on=['STFIPS', 'PlaceName'], right_on=['STATE', 'NAME'], how='left')\n",
    "\n",
    "#Dropping unneeded columns\n",
    "#Note: could drop many more columns in the future to save space, esp when scaling up\n",
    "#Leaving other columns for now to allow for trouble-shooting\n",
    "df = df.drop(['CENSUS', 'STATE', 'NAME'], axis=1)\n",
    "\n",
    "#Creating flag for side of the street (even houses numbers = Right)\n",
    "df['StreetSide'] =  df.apply(side_function, axis=1)\n",
    "\n",
    "#Changing the whole dataframe to string and just the address numbers to integers to ensure future merges work.\n",
    "df = df.astype(str)\n",
    "df = df.astype({'AddressNumber': 'int'})\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#\n",
    "#         SECTION 3: Loading the GBF/DIME file to a dataframe and geocoding source data\n",
    "#\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#--------------------Loading GBF/DIME file-----------------------\n",
    "\n",
    "# NOTE: using Ann Arbor for testing...need to change to national file for full run\n",
    "# \n",
    "gbf = pd.read_csv('08378-0013.csv', dtype={'STPREDIR': object, 'STNAME': object, 'STTYPE': object, 'STSUFDIR': object, 'NONSTCO': object, 'MAPNO1': object, 'MAPNO2': object, 'MAPNO3': object,\n",
    "                                           'MAPNO4': object, 'COFLAG': object, 'LEFTADD1': object, 'LEFTADD2': object, 'RGTADD1': object, 'RGTADD2': object, 'CENTRA1': object, 'CENTRA2': object,\n",
    "                                           'CENTRA3': object, 'CENTRA4': object, 'ZIPCOLEF': object, 'ZIPCORGT': object, 'SMSA': object, 'PLACO1': object, 'PLACO2': object, 'STCOLEFT': object, \n",
    "                                           'CNTCOLFT': object, 'MCDCCDL': object, 'BLKLEFT': object, 'STCORGT': object, 'CNTCORGT': object, 'MCDCCDR': object, 'BLOCKRGT': object, \n",
    "                                           'STPLACO1': object, 'STPLACO2': object, 'FROMLAT': object, 'FROMLONG': object, 'TOLAT': object, 'TOLONG': object, 'STPLA1': object, 'STPLA2': object,\n",
    "                                           'STPLA3': object, 'STPLA4': object, 'LEFTADD3': object, 'LEFTADD4': object, 'RGTADD3': object, 'RGTADD4': object})\n",
    "\n",
    "#removing the descriptive row of the dataframe...it's really not needed, messes up merges, and could be removed from the initial file setup\n",
    "gbf = gbf.drop([0], axis=0)\n",
    "#removing the rows without address numbers because they are not useful for this project and tend to be records of political boundaries, rivers, freeways, etc.\n",
    "gbf = gbf.dropna(subset=['LEFTADD1', 'RGTADD1'])\n",
    "\n",
    "#Need to recode Avenues because gecode file uses \"AV\" and source file uses \"AVE\"\n",
    "#Depending on the source file, this might not be needed or might be more efficient to change the source file dataframe.\n",
    "gbf['STTYPE'].replace({'AV': 'AVE'}, inplace = True)\n",
    "\n",
    "#Changing the whole dataframe to string and just the address numbers to int to ensure future merges work.\n",
    "gbf = gbf.astype(str)\n",
    "gbf = gbf.astype({'LEFTADD1': 'int', 'LEFTADD2': 'int', 'RGTADD1': 'int', 'RGTADD2': 'int'})\n",
    "\n",
    "\n",
    "#--------------------Merging the source data and the GBF/DIME data-----------------------\n",
    "\n",
    "# Note: the GBF/DIME file is divided into sides of the street (since Census tracts often use streets as borders/boundaries)\n",
    "\n",
    "\n",
    "#Geocoding the right side of the street first\n",
    "dfr = df[df['StreetSide']=='Right']\n",
    "\n",
    "#Since the GBF/DIME file uses address number ranges, numpy is used to merge the two data frames based on those ranges as well as Boolean comparisons\n",
    "#(reference: https://stackoverflow.com/questions/44367672/best-way-to-join-merge-by-range-in-pandas)\n",
    "addnum = dfr.AddressNumber.to_numpy()\n",
    "maplow = gbf.RGTADD1.to_numpy()\n",
    "maphigh = gbf.RGTADD2.to_numpy()\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfr.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfr.StreetNamePreDirectional.to_numpy()[:, None] == gbf.STPREDIR.to_numpy())\n",
    "                & (dfr.StreetNameSuffix.to_numpy()[:, None] == gbf.STTYPE.to_numpy())\n",
    "                & (dfr.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfr.PLACE.to_numpy()[:, None] == gbf.PLACO2.to_numpy())\n",
    "                & (dfr.STFIPS.to_numpy()[:, None] == gbf.STCORGT.to_numpy()))\n",
    "\n",
    "df_range_r = pd.DataFrame(\n",
    "    np.column_stack([dfr.values[i], gbf.values[j]]),\n",
    "    columns=dfr.columns.append(gbf.columns)).set_index(['New_ID'])\n",
    "\n",
    "\n",
    "# Iterating the records that didn't match above, based on missing values in search keys\n",
    "\n",
    "\n",
    "#Subset what records were not merged above\n",
    "dfr_remain = dfr[~dfr.index.isin(df_range_r.index)]\n",
    "# If there is no street type...\n",
    "dfr_next = dfr_remain[dfr_remain['StreetNameSuffix']=='']\n",
    "addnum = dfr_next.AddressNumber.to_numpy()\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfr_next.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfr_next.StreetNamePreDirectional.to_numpy()[:, None] == gbf.STPREDIR.to_numpy())\n",
    "                & (dfr_next.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfr_next.PLACE.to_numpy()[:, None] == gbf.PLACO2.to_numpy())\n",
    "                & (dfr_next.STFIPS.to_numpy()[:, None] == gbf.STCORGT.to_numpy()))\n",
    "\n",
    "df_range_r = df_range_r.append(pd.DataFrame(\n",
    "    np.column_stack([dfr_next.values[i], gbf.values[j]]),\n",
    "    columns=dfr_next.columns.append(gbf.columns)).set_index(['New_ID']))\n",
    "\n",
    "#Subset what records were not merged above\n",
    "dfr_remain = dfr[~dfr.index.isin(df_range_r.index)]\n",
    "# If there is no street direction prefix...\n",
    "dfr_next = dfr_remain[dfr_remain['StreetNamePreDirectional']=='']\n",
    "addnum = dfr_next.AddressNumber.to_numpy()\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfr_next.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfr_next.StreetNameSuffix.to_numpy()[:, None] == gbf.STTYPE.to_numpy())\n",
    "                & (dfr_next.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfr_next.PLACE.to_numpy()[:, None] == gbf.PLACO2.to_numpy())\n",
    "                & (dfr_next.STFIPS.to_numpy()[:, None] == gbf.STCORGT.to_numpy()))\n",
    "\n",
    "df_range_r = df_range_r.append(pd.DataFrame(\n",
    "    np.column_stack([dfr_next.values[i], gbf.values[j]]),\n",
    "    columns=dfr_next.columns.append(gbf.columns)).set_index(['New_ID']))\n",
    "\n",
    "# There is definitely room here to add code for other missing data or to add other data elements.\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "#Repeating all of the above with the left side of the street now\n",
    "\n",
    "dfl = df[df['StreetSide']=='Left']\n",
    "\n",
    "addnum = dfl.AddressNumber.to_numpy()\n",
    "maplow = gbf.LEFTADD1.to_numpy()\n",
    "maphigh = gbf.LEFTADD2.to_numpy()\n",
    "\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfl.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfl.StreetNamePreDirectional.to_numpy()[:, None] == gbf.STPREDIR.to_numpy())\n",
    "                & (dfl.StreetNameSuffix.to_numpy()[:, None] == gbf.STTYPE.to_numpy())\n",
    "                & (dfl.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfl.PLACE.to_numpy()[:, None] == gbf.PLACO1.to_numpy())\n",
    "                & (dfl.STFIPS.to_numpy()[:, None] == gbf.STCOLEFT.to_numpy()))\n",
    "\n",
    "df_range_l = pd.DataFrame(\n",
    "    np.column_stack([dfl.values[i], gbf.values[j]]),\n",
    "    columns=dfl.columns.append(gbf.columns)).set_index(['New_ID'])\n",
    "\n",
    "\n",
    "# Iterating the records that didn't match above, based on missing values in search keys\n",
    "\n",
    "\n",
    "#Subset what records were not merged above\n",
    "dfl_remain = dfl[~dfl.index.isin(df_range_l.index)]\n",
    "# If there is no street type...\n",
    "dfl_next = dfl_remain[dfl_remain['StreetNameSuffix']=='']\n",
    "addnum = dfl_next.AddressNumber.to_numpy()\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfl_next.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfl_next.StreetNamePreDirectional.to_numpy()[:, None] == gbf.STPREDIR.to_numpy())\n",
    "                & (dfl_next.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfl_next.PLACE.to_numpy()[:, None] == gbf.PLACO1.to_numpy())\n",
    "                & (dfl_next.STFIPS.to_numpy()[:, None] == gbf.STCOLEFT.to_numpy()))\n",
    "\n",
    "df_range_l = df_range_l.append(pd.DataFrame(\n",
    "    np.column_stack([dfl_next.values[i], gbf.values[j]]),\n",
    "    columns=dfl_next.columns.append(gbf.columns)).set_index(['New_ID']))\n",
    "\n",
    "#Subset what records were not merged above\n",
    "dfl_remain = dfl[~dfl.index.isin(df_range_l.index)]\n",
    "# If there is no street direction prefix...\n",
    "dfl_next = dfl_remain[dfl_remain['StreetNamePreDirectional']=='']\n",
    "addnum = dfl_next.AddressNumber.to_numpy()\n",
    "\n",
    "i, j = np.where((addnum[:, None] >= maplow) & (addnum[:, None] <= maphigh) \n",
    "                & (dfl_next.StreetName.to_numpy()[:, None] == gbf.STNAME.to_numpy())\n",
    "                & (dfl_next.StreetNameSuffix.to_numpy()[:, None] == gbf.STTYPE.to_numpy())\n",
    "                & (dfl_next.StreetNamePostDirectional.to_numpy()[:, None] == gbf.STSUFDIR.to_numpy())\n",
    "                & (dfl_next.PLACE.to_numpy()[:, None] == gbf.PLACO1.to_numpy())\n",
    "                & (dfl_next.STFIPS.to_numpy()[:, None] == gbf.STCOLEFT.to_numpy()))\n",
    "\n",
    "df_range_l = df_range_l.append(pd.DataFrame(\n",
    "    np.column_stack([dfl_next.values[i], gbf.values[j]]),\n",
    "    columns=dfl_next.columns.append(gbf.columns)).set_index(['New_ID']))\n",
    "\n",
    "\n",
    "#Appending the two sides of the street into a single dataframe again\n",
    "df_geocode = df_range_l.append(df_range_r).sort_index()\n",
    "\n",
    "\n",
    "#Print to see if it looks as expected (omit if dataframe is too large to print efficiently)\n",
    "print(df_geocode.to_string())\n",
    "\n",
    "\n",
    "\n",
    "#Check if there are unmatched records:\n",
    "print(df[~df.index.isin(df_geocode.index)].to_string())\n",
    "\n",
    "\n",
    "#---- Here is the start of how to append all the records from the original df that are not in the current df:\n",
    "# Reference: https://stackoverflow.com/questions/44367672/best-way-to-join-merge-by-range-in-pandas\n",
    "\n",
    "\n",
    "#df_geocode = df_geocode.append(\n",
    "#    df[~np.in1d(np.arange(len(df)), np.unique(i))],\n",
    "#    ignore_index=False, sort=False\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
